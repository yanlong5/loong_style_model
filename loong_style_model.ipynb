{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdQU55iaZ-Eq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class CustomConcat(Layer):\n",
        "    def __init__(self):\n",
        "        super(CustomConcat, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.concat(inputs, -1)\n",
        "        \n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return inputs[0]._keras_mask\n",
        "\n",
        "class CustomDot(Layer):\n",
        "    def __init__(self):\n",
        "        super(CustomDot, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs[0], inputs[1])\n",
        "        \n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return inputs[0]._keras_mask\n",
        "\n",
        "class CustomActivation(Layer):\n",
        "    def __init__(self):\n",
        "        super(CustomActivation, self).__init__()\n",
        "        self.activation = tf.keras.layers.Activation(\"relu\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        s1, s2, s3 = tf.split(inputs, num_or_size_splits=3, axis=-1)\n",
        "        s2 = self.activation(s2)\n",
        "        output = tf.concat([s1, s2, s3], axis=-1)\n",
        "        return output\n",
        "        \n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "# This adds positional encoding as integers to the embeddings\n",
        "class PositionalIndexAppender(Layer):\n",
        "    def __init__(self):\n",
        "        super(PositionalIndexAppender, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, sequence_length, embedding_dim)\n",
        "        batch_size, sequence_length, _ = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2]\n",
        "        \n",
        "        # Generate positional indices for the sequence length and tile for each batch\n",
        "        positions = tf.range(1, sequence_length + 1, dtype=tf.float32)  # Shape: [sequence_length]\n",
        "        positions = tf.reshape(positions, [1, sequence_length, 1])      # Shape: [1, sequence_length, 1]\n",
        "        positions = tf.tile(positions, [batch_size, 1, 1])              # Shape: [batch_size, sequence_length, 1]\n",
        "\n",
        "        # Concatenate along the last dimension\n",
        "        return tf.concat([inputs, positions], axis=-1)\n",
        "        \n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "\n",
        "class LoongStyleUnit(Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LoongStyleUnit, self).__init__()\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        self.kernel_1 = self.add_weight(shape=(self.state_size + input_dim, self.state_size))\n",
        "        self.bias_1 = self.add_weight(shape=(self.state_size,))\n",
        "        self.kernel_2 = self.add_weight(shape=(self.state_size * 2, self.state_size))\n",
        "        self.bias_2 = self.add_weight(shape=(self.state_size,))\n",
        "        self.kernel_3 = self.add_weight(shape=(self.state_size, self.state_size))\n",
        "        self.bias_3 = self.add_weight(shape=(self.state_size,))\n",
        "        self.kernel_4 = self.add_weight(shape=(input_dim, self.state_size))\n",
        "        self.bias_4 = self.add_weight(shape=(self.state_size,))\n",
        "        self.kernel_5 = self.add_weight(shape=(self.state_size * 2, self.state_size))\n",
        "        self.bias_5 = self.add_weight(shape=(self.state_size,))\n",
        "        self.kernel_6 = self.add_weight(shape=(self.state_size * 2, self.state_size))\n",
        "        self.bias_6 = self.add_weight(shape=(self.state_size,))\n",
        "        self.kernel_7 = self.add_weight(shape=(self.state_size, self.state_size))\n",
        "        self.bias_7 = self.add_weight(shape=(self.state_size,))\n",
        "        super(LoongStyleUnit, self).build(input_shape)\n",
        "\n",
        "    def get_initial_state(self, batch_size=None):\n",
        "        return [tf.zeros((batch_size, self.state_size))] \n",
        "    \n",
        "    def call(self, inputs, states):\n",
        "        states = states[0]\n",
        "\n",
        "        # This part is similar to LSTM, but use more steps to generate values of gate.\n",
        "        # It first does a linear transformation with the inputs and previous hidden states.\n",
        "        # if we only do linear transformation once, no matter what the hidden states are, the model will do the same operation to the new inputs, and add that results to generate output.\n",
        "        # That means when we process inputs, we do not consider the previous context. That is why LSTM perform better than basic RNN, because it uses gate instead of generating values directly.\n",
        "        # In this RNN unit, it pushes this further. It embeds the information from both sides to the result of the first linear transformation.\n",
        "        # The model then look at that result and the original information from one side to give a value. Finaly, it takes the third linear transformation by itself, which will be used as the gate value.\n",
        "        # After each linear transformation, it does a layer normalization step.\n",
        "        # This part acts as an activation function, otherwise all these steps can be summarized and expresssed by one linear transformation, which makes all the effort usesless. \n",
        "        v1 = tf.matmul(tf.concat([states, inputs], axis=-1), self.kernel_1) + self.bias_1\n",
        "        v1_mean, v1_variance = tf.nn.moments(v1, axes=[-1], keepdims=True)\n",
        "        v1 = (v1 - v1_mean) / tf.sqrt(v1_variance + 0.001)\n",
        "\n",
        "        v2 = tf.matmul(tf.concat([v1, states], axis=-1), self.kernel_2) + self.bias_2\n",
        "        v2_mean, v2_variance = tf.nn.moments(v2, axes=[-1], keepdims=True)\n",
        "        v2 = (v2 - v2_mean) / tf.sqrt(v2_variance + 0.001)\n",
        "\n",
        "        v3 = tf.matmul(v2, self.kernel_3) + self.bias_3\n",
        "\n",
        "        s1 = tf.matmul(inputs, self.kernel_4) + self.bias_4\n",
        "\n",
        "        s2 = tf.matmul(tf.concat([states, s1], axis=-1), self.kernel_5) + self.bias_5\n",
        "        s2_mean, s2_variance = tf.nn.moments(s2, axes=[-1], keepdims=True)\n",
        "        s2 = (s2 - s2_mean) / tf.sqrt(s2_variance + 0.001)\n",
        "\n",
        "        s3 = tf.matmul(tf.concat([s2, s1], axis=-1), self.kernel_6) + self.bias_6\n",
        "        s3_mean, s3_variance = tf.nn.moments(s3, axes=[-1], keepdims=True)\n",
        "        s3 = (s3 - s3_mean) / tf.sqrt(s3_variance + 0.001)\n",
        "\n",
        "        s4 = tf.matmul(s3, self.kernel_7) + self.bias_7\n",
        "        \n",
        "        final = v3 * states + s4 * s1\n",
        "        mean, variance = tf.nn.moments(final, axes=[-1], keepdims=True)\n",
        "        final = (final - mean) / tf.sqrt(variance + 0.001)\n",
        "        \n",
        "        return final, [final]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAQp6sbKaLYF"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    pos_append = PositionalIndexAppender()\n",
        "    custom_dot = CustomDot()\n",
        "    activation = CustomActivation()\n",
        "    custom_concat = CustomConcat()\n",
        "\n",
        "    # In Transformer architicture, it uses weights to select values to generate new value, but if you do it this way, the new value is not percise enough. \n",
        "    # Assume the input vectors has length N. In this method, It first uses a special RNN unit to go over all the inputs of the sequence, and generate an embedding with length M.\n",
        "    # and it does a linear transformation using this embedding with a matirx of shape (N X N) X  M.\n",
        "    # Next, reshape the resulting vector to a matrix with shape N x N. This matrix is dynamic, its values depends on the inputs, whereas the previous (N X N) X  M is fixed and trained.\n",
        "    # Then, times all input vectors with the matrix to output new vectors with length N.\n",
        "    # All the steps above is one layer of the structure, and can be repeated many times.\n",
        "    # After several layers, concatanate the output of all the layers. if you have Z layers, the length of the new vector will be ZN.\n",
        "    # Finally, use the special RNN unit to process the whole sequence to give the final result(after adding several Dense layers).\n",
        "    def loong_style_layer(input):\n",
        "        info = tf.keras.layers.RNN(cell=LoongStyleUnit(150))(input)\n",
        "        matrix = tf.keras.layers.Dense(9900)(info)\n",
        "        matrix = tf.keras.layers.Reshape((100,99))(matrix)\n",
        "        output = custom_dot([input, matrix])\n",
        "        output = tf.keras.layers.LayerNormalization(center=False, scale=False)(output)\n",
        "        output = activation(output)\n",
        "        output = tf.keras.layers.Dropout(0.2)(output)\n",
        "        return output\n",
        "    \n",
        "    input = tf.keras.Input(shape = (None,))\n",
        "    output = tf.keras.layers.Embedding(10000, 99, mask_zero=True)(input)\n",
        "    output = pos_append(output)\n",
        "    outputs = []\n",
        "    for i in range(12):\n",
        "        output = loong_style_layer(output)\n",
        "        outputs.append(output)\n",
        "        output = pos_append(output)\n",
        "    output = custom_concat(outputs)\n",
        "    output = pos_append(output)\n",
        "    output = tf.keras.layers.RNN(cell=LoongStyleUnit(1200))(output)\n",
        "    output = activation(output)\n",
        "    output = tf.keras.layers.Dense(1000, activation='softmax')(output)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
